{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "305rhqmosM3w"
      },
      "outputs": [],
      "source": [
        "#!pip install -U huggingface_hub scikit-learn pandas numpy --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "\n",
        "\n",
        "client = InferenceClient(\n",
        "    model=\"deepseek-ai/DeepSeek-R1\",\n",
        "    token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "qs32cPCg3d-F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "def generate_metrics():\n",
        "    df = pd.DataFrame({\n",
        "        \"cpu\": np.random.normal(50, 10, 100),\n",
        "        \"memory\": np.random.normal(60, 15, 100),\n",
        "        \"disk_io\": np.random.normal(100, 25, 100),\n",
        "    })\n",
        "    df.iloc[95:100] += np.random.uniform(100, 200)\n",
        "    return df\n",
        "\n",
        "def detect_anomalies(df):\n",
        "    model = IsolationForest(contamination=0.05, random_state=42)\n",
        "    model.fit(df)\n",
        "    df[\"anomaly\"] = model.predict(df)\n",
        "    return df[df[\"anomaly\"] == -1]\n"
      ],
      "metadata": {
        "id": "WxuBQJto7Eej"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_messages(anomalies_df):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": \"You are a senior DevOps engineer helping to explain system anomalies.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"Anomalies were detected in the following system metrics:\n",
        "\n",
        "{anomalies_df.to_string(index=False)}\n",
        "\n",
        "Explain:\n",
        "1. What might have caused these?\n",
        "2. What is the likely impact?\n",
        "3. How should we fix it?\n",
        "\"\"\"}\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "o1Dhz4Ge7GTD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def evaluate_response(text, start_time):\n",
        "    duration = round(time.time() - start_time, 2)\n",
        "    words = len(text.split())\n",
        "    keywords = [\"cpu\", \"memory\", \"disk\", \"restart\", \"fix\", \"usage\", \"overload\"]\n",
        "    relevance_hits = sum(k in text.lower() for k in keywords)\n",
        "    relevance_score = round(relevance_hits / len(keywords), 2)\n",
        "    return {\n",
        "        \"latency_seconds\": duration,\n",
        "        \"word_count\": words,\n",
        "        \"relevance_score\": relevance_score\n",
        "    }\n"
      ],
      "metadata": {
        "id": "7Y2dsPsH7Iej"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = generate_metrics()\n",
        "anomalies = detect_anomalies(df)\n",
        "\n",
        "if anomalies.empty:\n",
        "    print(\"âœ… No anomalies detected.\")\n",
        "else:\n",
        "    print(\"ðŸš¨ Anomalies found. Sending to DeepSeek...\\n\")\n",
        "    messages = build_messages(anomalies)\n",
        "\n",
        "    start = time.time()\n",
        "    response = client.chat_completion(messages=messages)\n",
        "    reply = response.choices[0].message.content.strip()\n",
        "\n",
        "    print(\" DeepSeek Response:\\n\", reply)\n",
        "\n",
        "    metrics = evaluate_response(reply, start)\n",
        "    print(\"\\n Evaluation Metrics:\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\" - {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "S_XAJ4I07J4e",
        "outputId": "45923297-ca35-4b98-d1a7-e47a59489a6d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš¨ Anomalies found. Sending to DeepSeek...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " DeepSeek Response:\n",
            " <think>\n",
            "Okay, let me try to work through this problem. So, there's a dataset with system metrics for CPU, memory, disk I/O, and an anomaly flag. All the rows here are marked as anomalies with -1. My task is to figure out what's causing these anomalies, their impact, and how to fix them. Let's break it down step by step.\n",
            "\n",
            "First, looking at the numbers. The CPU seems to be in the 237-253 range, memory is from about 242 to 279, disk I/O is the highest at around 273-318. Wait, the units aren't specified, which could be an issue. But typically, CPU usage is a percentage, so maybe here it's scaled? If those are percentages, CPU at 237% would indicate a system with more than one core since 100% per core is max. So if it's a multi-core system, maybe each CPU is maxed out. But 237% might mean 2.37 cores fully utilized. But that's still high. Memory here, if it's in MB or GB, but 242 seems high if it's GB. Maybe it's in MB, which would be 242MB, which is low. Wait, but then anomaly is marked. Maybe these are normalized scores or scaled values. Hmm, perhaps the numbers are standardized (z-scores) instead of raw metrics. That's possible because they all are in similar ranges despite different units. If they're z-scores, then values above a certain threshold (like Â±2 or 3) mean anomalies. But the user hasn't specified that, so maybe I need to work without knowing.\n",
            "\n",
            "Assuming the metrics are in their original units, but the actual numbers don't make sense. Wait, 310 for disk I/Oâ€”if that's IOPS (input/output operations per second), that's possible, but depends on the disk. Or if it's throughput in MB/s, 300MB/s is high for HDDs but achievable on SSDs. But unless given more context, perhaps the anomaly is due to the combined factors. So let's look for patterns.\n",
            "\n",
            "Looking at the data:\n",
            "\n",
            "Row 1: CPU 237, memory 242, disk I/O 310\n",
            "Row 2: CPU 240, memory 262, disk I/O 313\n",
            "Row 3: CPU 253, memory 245, disk I/O 318\n",
            "Row 4: CPU 242, memory 279, disk I/O 313\n",
            "Row 5: CPU 247, memory 279, disk I/O 273\n",
            "\n",
            "So over time, maybe across 5 time intervals. Not sure about the intervals. Let's see the trends. CPU has a peak in the third row, then comes down but is still high. Memory starts around 242, then increases to 279 in rows 4 and 5. Disk I/O starts high at 310, peaks at 318, then in the last row drops to 273. So in the last row, disk I/O decreases but CPU and memory remain high.\n",
            "\n",
            "Possible scenarios:\n",
            "\n",
            "1. CPU and Memory are both high. Could be a memory leak causing processes to consume more memory, leading to higher CPU due to garbage collection or swapping. Wait, but if memory is being exhausted, the system might start swapping to disk, which would increase disk I/O. However, here in the first four rows, disk I/O is high. Then in the last row, disk I/O drops, while memory is still high. Maybe initially, high memory usage leading to swapping (high disk I/O), then when memory is exhausted, perhaps processes are getting killed or the system is freezing, leading to reduced disk I/O? Or maybe the app started using more memory and less disk, but need to see.\n",
            "\n",
            "But the combination of high CPU, high memory, and high disk I/O in the first four rows suggests a resource-heavy process. For example, a process that's both CPU-intensive and memory-intensive, writing a lot to disk. High disk I/O could also indicate swapping if memory is full.\n",
            "\n",
            "Wait, when memory is full, the system uses swap space on disk, which is slow. So high memory usage + high disk I/O (from swapping) could cause high CPU because the system is spending time managing memory and swapping. That makes sense. So if the system is using swap, disk I/O would be high (due to page-ins/outs), CPU would be high due to handling the swapping, and memory is maxed out.\n",
            "\n",
            "But in rows 1-4, memory is increasing from 242 to 279. Let's say memory is measured in GB, but a system with 279GB of memory is quite big. If these are percentages, maybe the memory usage is at 242% of a threshold? That seems odd. Alternatively, if these are scaled metrics (like normalized values for monitoring purposes), then being above a certain threshold flags anomaly.\n",
            "\n",
            "Alternatively, perhaps a misconfigured application is consuming increasing memory, leading to swapping, hence high disk I/O. Then, in the last entry, disk I/O drops but memory is still high. Maybe the system ran out of swap space, so swapping stops, leading to OOM (out of memory) errors, but the anomaly detection might still flag that state. Alternatively, if memory was addressed and some processes were killed, but CPU remains high because the system is recovering?\n",
            "\n",
            "Alternatively, if disk I/O is the main driver here. Let's think. High disk I/O could be due to a lot of read/write operations. If an application is writing logs excessively, or there's a database doing heavy transactions. But high disk I/O plus high CPU might indicate the application is processing a lot of data (using CPU) and reading/writing to disk. High memory might be caching? For example, a database using a lot of memory for cache, which would improve performance, but if the data is large, it might require a lot of disk I/O as well. If the memory is filled with cache, but the application still needs to access disk for some data, then high memory and disk I/O together. However, usually, effective caching reduces disk I/O. So conflicting ideas here.\n",
            "\n",
            "Another angle: the combination of all three metrics being high suggests an overlapping workload. Maybe a memory leak in an application that's also CPU intensive and writing to disk. But the resource usage is spiking, leading to the anomaly detection.\n",
            "\n",
            "Now, regarding the possible fixes. If it's a memory leak, restarting the process might help. Scaling up resources (adding more CPU, memory) could mitigate, but not fix the root cause. Optimizing application code, checking for memory leaks, or tuning garbage collection settings (if applicable) would be better. For high disk I/O, checking what's causing the disk usage (i.e., using tools like iotop), and maybe optimizing queries, batch processing, or using faster storage.\n",
            "\n",
            "Impact: The high CPU and memory could lead to slow response times, timeouts, or service crashes. High disk I/O could cause latency in disk operations, affecting applications that rely on disk access. If swapping is happening, the system would be significantly slower.\n",
            "\n",
            "Potential Causes:\n",
            "\n",
            "- A memory leak in a process causing increased memory usage over time, leading to swapping (high disk I/O as pages are swapped in/out). The swapping would cause higher CPU usage as the system handles paging.\n",
            "\n",
            "- A runaway process that is consuming CPU, memory, and performing heavy disk I/O. For example, a data processing job that's not optimized, maybe doing large file operations while keeping data in memory.\n",
            "\n",
            "- A database that's under heavy load with insufficient indexing, causing full table scans (high CPU and disk I/O) and using a lot of memory for caching.\n",
            "\n",
            "- A logging service that's writing excessively to disk, coupled with a CPU-heavy task and memory-intensive operations.\n",
            "\n",
            "Possible Steps to Fix:\n",
            "\n",
            "1. Identify the processes consuming the most CPU, memory, and disk I/O. Tools like top, htop, vmstat, iostat, iotop can help.\n",
            "\n",
            "2. Check for memory leaks: Monitor memory usage over time. If it keeps increasing without release, a process might have a leak.\n",
            "\n",
            "3. Analyze disk I/O: Check what files/directories are being written to/read from. Database logs, application logs, or temporary files could be culprits.\n",
            "\n",
            "4. Check swap usage (with 'free' or 'vmstat') to see if the system is swapping. High swap usage indicates insufficient physical memory.\n",
            "\n",
            "5. If swapping is occurring, consider adding more RAM or optimizing memory usage in applications.\n",
            "\n",
            "6. For CPU usage, look into optimizing the code, checking for inefficient loops, or distributing the load.\n",
            "\n",
            "7. For disk I/O, consider using faster storage (SSDs), optimizing I/O operations (e.g., batch writes), or scaling horizontally.\n",
            "\n",
            "Impact:\n",
            "\n",
            "- System performance degradation, leading to slower application response.\n",
            "\n",
            "- Potential service unavailability if memory is exhausted (OOM killer might terminate processes).\n",
            "\n",
            "- Increased latency in disk operations affecting applications relying on disk access.\n",
            "\n",
            "- Possible data loss or corruption if the system crashes due to resource exhaustion.\n",
            "\n",
            "So putting it all together:\n",
            "\n",
            "The anomalies in CPU, memory, and disk I/O are likely due to a resource-intensive process causing high utilization across all three metrics. The most probable root cause is a memory leak leading to increased memory usage, which in turn triggers excessive swapping (high disk I/O) and increased CPU usage from managing the paging. Alternatively, a badly behaving process could be consuming CPU, memory, and disk I/O simultaneously. To resolve, investigate process-level metrics, check for swapping, and address the underlying application issue causing the resource exhaustion.\n",
            "</think>\n",
            "\n",
            "### Analysis of System Anomalies\n",
            "\n",
            "#### 1. **Potential Causes**\n",
            "The anomalies likely stem from **a memory-intensive process triggering cascading resource exhaustion**:\n",
            "- **Memory Leak or Overflow**:\n",
            "  - Memory usage increases steadily\n",
            "\n",
            " Evaluation Metrics:\n",
            " - latency_seconds: 22.54\n",
            " - word_count: 1530\n",
            " - relevance_score: 0.86\n"
          ]
        }
      ]
    }
  ]
}